import json
import hashlib
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import subprocess, os
import httpx 
import re

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
UPLOAD_DIR = os.path.join(BASE_DIR, "uploads")
RESULT_DIR = os.path.join(BASE_DIR, "results")

os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(RESULT_DIR, exist_ok=True)

app.mount("/results", StaticFiles(directory="results"), name="results")

# ===== HF Space è¨­å®š =====
HF_SPACE_BASE = "https://raxhel-codebert-Malvec.hf.space"  # âœ… ä¿®æ­£: å¤§å¯« M
HF_TOKEN = os.environ.get("hf_token")  # å¯é¸,å¦‚æœ Space æ˜¯ç§æœ‰çš„

async def trigger_hf_prediction(filename: str):
    """
    è§¸ç™¼ HF Space é æ¸¬ä¸¦å–å¾—çµæœ
    æ”¯æ´åˆ†æ‰¹ä¸Šå‚³å¤§é‡æª”æ¡ˆ
    
    Args:
        filename: æª”æ¡ˆåç¨± (ä¾‹å¦‚: "test.exe")
    
    Returns:
        dict: åŒ…å« final_label å’Œ embedding çš„é æ¸¬çµæœ
    """
    
    # æ‰¾åˆ°åˆ†æ®µçš„ TXT æª”æ¡ˆ
    safe_filename = re.sub(r'[^\w\-]', '_', filename.rsplit('.', 1)[0])
    segment_dir = os.path.join(RESULT_DIR, "separate", f"unpacked_{safe_filename}")
    
    if not os.path.exists(segment_dir):
        print(f"âŒ Segment directory not found: {segment_dir}")
        return None
    
    # è®€å–æ‰€æœ‰ TXT æª”æ¡ˆ
    txt_files = sorted([f for f in os.listdir(segment_dir) if f.endswith('.txt')])
    
    if not txt_files:
        print(f"âŒ No TXT files found in: {segment_dir}")
        return None
    
    print(f"ğŸ“‚ Found {len(txt_files)} segment files")
    
    # âœ… åˆ†æ‰¹è™•ç† - æ¯æ‰¹æœ€å¤š 50 å€‹æª”æ¡ˆ
    BATCH_SIZE = 50
    batches = [txt_files[i:i + BATCH_SIZE] for i in range(0, len(txt_files), BATCH_SIZE)]
    
    print(f"ğŸ“¦ Splitting into {len(batches)} batches of up to {BATCH_SIZE} files each")
    
    predict_url = f"{HF_SPACE_BASE}/predict"
    
    # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„çµæœ
    all_predictions = []
    all_embeddings = []
    
    try:
        async with httpx.AsyncClient(timeout=300.0) as client:
            headers = {}
            if HF_TOKEN:
                headers["Authorization"] = f"Bearer {HF_TOKEN}"
            
            # å…ˆç¢ºèª Space æ˜¯å¦é‹è¡Œ
            print(f"ğŸ”„ Checking Space status...")
            try:
                status_response = await client.get(f"{HF_SPACE_BASE}/model-info", timeout=10.0)
                print(f"   Status check response: {status_response.status_code}")
                if status_response.status_code == 200:
                    model_info = status_response.json()
                    print(f"âœ… Space is running: {model_info.get('model_name')}")
                else:
                    print(f"âš ï¸  Space returned {status_response.status_code}, continuing anyway...")
            except Exception as e:
                print(f"âš ï¸  Could not verify Space status: {e}")
                print(f"   Continuing anyway...")
            
            # âœ… é€æ‰¹è™•ç†
            for batch_idx, batch_files in enumerate(batches, 1):
                print(f"\nğŸ“¤ Processing batch {batch_idx}/{len(batches)} ({len(batch_files)} files)...")
                print(f"   Target URL: {predict_url}")
                
                # æº–å‚™é€™æ‰¹æª”æ¡ˆ
                files = []
                for txt_file in batch_files:
                    file_path = os.path.join(segment_dir, txt_file)
                    with open(file_path, 'rb') as f:
                        content = f.read()
                    files.append(('files', (txt_file, content, 'text/plain')))
                
                print(f"   Prepared {len(files)} files for upload")
                
                # ç™¼é€è«‹æ±‚
                try:
                    print(f"   ğŸŒ Sending POST request...")
                    response = await client.post(
                        predict_url,
                        files=files,
                        headers=headers,
                        timeout=300.0
                    )
                    
                    print(f"   ğŸ“¥ Response received: {response.status_code}")
                    
                    if response.status_code != 200:
                        print(f"   âŒ Batch {batch_idx} failed: {response.status_code}")
                        print(f"   Response headers: {dict(response.headers)}")
                        print(f"   Response body (first 500 chars): {response.text[:500]}")
                        continue
                    
                    batch_result = response.json()
                    
                    # æ”¶é›†é€™æ‰¹çš„é æ¸¬çµæœ
                    if "segment_predictions" in batch_result:
                        all_predictions.extend(batch_result["segment_predictions"])
                        print(f"   âœ… Batch {batch_idx} completed: {batch_result.get('final_label')}")
                        print(f"   Added {len(batch_result['segment_predictions'])} predictions")
                    else:
                        print(f"   âš ï¸  Batch {batch_idx}: No segment_predictions in response")
                        print(f"   Response keys: {list(batch_result.keys())}")
                    
                except httpx.TimeoutError as e:
                    print(f"   â±ï¸  Batch {batch_idx} timeout: {e}")
                    continue
                except httpx.RequestError as e:
                    print(f"   âŒ Batch {batch_idx} request error: {e}")
                    continue
                except Exception as e:
                    print(f"   âŒ Batch {batch_idx} unexpected error: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        if not all_predictions:
            print("âŒ No predictions received from any batch")
            return None
        
        # âœ… åˆä½µæ‰€æœ‰æ‰¹æ¬¡çš„çµæœ
        print(f"\nğŸ“Š Merging results from {len(all_predictions)} predictions...")
        
        # çµ±è¨ˆæ‰€æœ‰é æ¸¬çš„æ¨™ç±¤
        from collections import Counter
        label_votes = [p["predicted_label"] for p in all_predictions]
        vote_counts = Counter(label_votes)
        final_label = vote_counts.most_common(1)[0][0]
        final_count = vote_counts[final_label]
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
        avg_confidence = sum(p["confidence"] for p in all_predictions) / len(all_predictions)
        
        # é¸æ“‡æ³¨æ„åŠ›æœ€é«˜ä¸”æ¨™ç±¤åŒ¹é…çš„ embedding
        same_label_preds = [p for p in all_predictions if p["predicted_label"] == final_label]
        if same_label_preds:
            best_pred = max(same_label_preds, key=lambda x: x.get("attention_score", 0))
        else:
            best_pred = max(all_predictions, key=lambda x: x.get("attention_score", 0))
        
        # æ§‹å»ºæœ€çµ‚çµæœ (éœ€è¦å¾åŸå§‹é æ¸¬ä¸­å–å¾— embedding)
        # æ³¨æ„: æˆ‘å€‘éœ€è¦é‡æ–°å–å¾— best_pred æ‰€åœ¨æ‰¹æ¬¡çš„å®Œæ•´ embedding è³‡è¨Š
        # é€™è£¡ç°¡åŒ–è™•ç†,è¿”å›çµ±è¨ˆçµæœ
        
        result = {
            "final_label": final_label,
            "vote_count": final_count,
            "total_segments": len(all_predictions),
            "confidence": avg_confidence,
            "vote_distribution": dict(vote_counts),
            "embedding": {
                "values": [0.0] * 768,  # ä½”ä½ç¬¦,éœ€è¦å®Œæ•´å¯¦ç¾
                "source_file": best_pred.get("filename", "unknown"),
                "attention_score": best_pred.get("attention_score", 0),
                "dimension": 768
            },
            "segment_predictions": all_predictions[:10]  # åªè¿”å›å‰ 10 å€‹ä½œç‚ºç¯„ä¾‹
        }
        
        print(f"âœ… Final prediction: {final_label} ({final_count}/{len(all_predictions)} votes)")
        print(f"   Confidence: {avg_confidence:.3f}")
        
        return result
            
    except Exception as e:
        print(f"âŒ Error calling HF Space: {e}")
        import traceback
        traceback.print_exc()
        return None

@app.post("/api/analyze")
async def analyze(file: UploadFile = File(...)):
    print("\n" + "="*80)
    print("ğŸš€ NEW REQUEST")
    print("="*80)
    print(f"ğŸ“ Filename: {file.filename}")
    
    # è®€å–ä¸¦é©—è­‰æª”æ¡ˆ
    content = await file.read()
    print(f"ğŸ“¦ Received: {len(content)} bytes")
    print(f"ğŸ” First 4 bytes: {content[:4].hex()}")
    
    # å„²å­˜æª”æ¡ˆ
    filename = file.filename
    upload_path = os.path.join(UPLOAD_DIR, filename)
    with open(upload_path, "wb") as f:
        f.write(content)
    
    print(f"ğŸ’¾ Saved to: {upload_path}")
    
    # è¨­å®šè¼¸å‡ºè·¯å¾‘
    disasm_csv = os.path.join(RESULT_DIR, f"{filename}_disasm.csv")
    details_json = os.path.join(RESULT_DIR, f"{filename}_details.json")
    unpacked_filename = f"unpacked_files/unpacked_{filename}"
    
    print("\nğŸ³ DOCKER EXECUTION")
    
    # Docker å‘½ä»¤
    docker_cmd = [
        "docker", "run", "--rm",
        "-v", f"{UPLOAD_DIR}:/mnt/project/input:ro",
        "-v", f"{RESULT_DIR}:/mnt/project/output",
        "final",
        "bash", "-c",
        (
            # Step 1: åŸ·è¡Œ unpack.py
            f"python /unpack.py /mnt/project/input/{filename} && "
            
            # Step 2: æª¢æŸ¥ JSON çš„ unpack_success
            f"UNPACK_SUCCESS=$(python -c \"import json; "
            f"data=json.load(open('/mnt/project/output/{filename}_details.json')); "
            f"print('true' if data.get('unpack_success') else 'false')\") && "
            
            f"if [ \"$UNPACK_SUCCESS\" = \"true\" ]; then "
            f"  echo 'âœ… Unpack successful, running disasm...' && "
            f"  python /disasm.py /mnt/project/output/{unpacked_filename} /mnt/project/output/{filename}_disasm.csv && "
            
            # Step 3: å¦‚æœ disasm CSV å­˜åœ¨,åŸ·è¡Œåˆ†æ®µ
            f"  if [ -f /mnt/project/output/{filename}_disasm.csv ]; then "
            f"    echo 'âœ… Disasm complete, segmenting...' && "
            f"    python /segment_disasm.py /mnt/project/output/{filename}_disasm.csv; "
            f"  fi; "
            f"else "
            f"  echo 'âš ï¸  Unpack failed or file not UPX packed, skipping disasm'; "
            f"fi"
        )
    ]
    
    print(f"Command: {' '.join(docker_cmd[:8])}...")
    
    try:
        result = subprocess.run(
            docker_cmd, 
            capture_output=True, 
            text=True, 
            timeout=120
        )
        
        print("\n" + "="*80)
        print("ğŸ“¤ DOCKER OUTPUT:")
        print("="*80)
        print(result.stdout)
        
        if result.stderr:
            print("\nâš ï¸  STDERR:")
            print(result.stderr)
        
        print("="*80)
        
    except subprocess.TimeoutExpired:
        print("âŒ Docker timeout!")
        return {
            "error": "Docker execution timeout",
            "filename": filename,
            "details": {"error": "Timeout after 120 seconds"}
        }
    except Exception as e:
        print(f"âŒ Docker error: {e}")
        return {
            "error": str(e),
            "filename": filename,
            "details": {"error": str(e)}
        }
    
    # è®€å–çµæœ
    print("\nğŸ“Š Reading results...")
    
    unpack_info = {}
    if os.path.exists(details_json):
        print(f"âœ… Found: {details_json}")
        with open(details_json, "r") as jf:
            try:
                unpack_info = json.load(jf)
                print(f"ğŸ“„ Details: {unpack_info}")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON error: {e}")
                unpack_info = {"error": "Invalid JSON"}
    else:
        print(f"âŒ Not found: {details_json}")
        unpack_info = {
            "error": "details.json not found",
            "is_pe32": False,
            "is_exe": False,
            "unpack_success": False
        }
    
    disasm_success = os.path.exists(disasm_csv)
    print(f"{'âœ…' if disasm_success else 'âŒ'} Disasm CSV: {disasm_success}")
    
    # ===== è§¸ç™¼ HF Space é æ¸¬ =====
    prediction_result = None
    if disasm_success and unpack_info.get("unpack_success"):
        print("\nğŸ¤— Triggering HF Space prediction...")
        prediction_result = await trigger_hf_prediction(filename)
        
        if prediction_result:
            print(f"âœ… Prediction successful: {prediction_result.get('final_label')}")
        else:
            print(f"âš ï¸  Prediction failed or unavailable")
    else:
        print("\nâš ï¸  Skipping prediction (disasm failed or not UPX packed)")
    
    response = {
        "filename": filename,
        "details": unpack_info,
        "disasm_csv": f"http://127.0.0.1:8000/results/{os.path.basename(disasm_csv)}" if disasm_success else None,
        "disasm_success": disasm_success,
        "status": "done" if disasm_success else "unpack_failed",
        "prediction": prediction_result  # åŒ…å« final_label å’Œ embedding
    }
    
    print("\nâœ… Response ready\n")
    return response

@app.get("/api/model-info")
async def get_model_info():
    """å–å¾— HF Space æ¨¡å‹è³‡è¨Š"""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{HF_SPACE_BASE}/model-info")
            if response.status_code == 200:
                return response.json()
            return {"status": "unavailable"}
    except:
        return {"status": "error"}

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "hf_space": HF_SPACE_BASE
    }